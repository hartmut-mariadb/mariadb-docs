# High Availability

{% include "https://app.gitbook.com/s/GxVnu02ec8KJuFSxmB93/~/reusable/6PNX0xBioKkO8lDBFeGN/" %}

This section provide guidance on how to run `MariaDB` and `MaxScale` in high availability mode. If you are looking to run the operator in HA as well, please refer to the [Helm documentation](installation/helm.md).

Our recommended HA setup for production is:

* [Galera](galera-cluster.md) with at least 3 nodes. Always an odd number of nodes.
* [MaxScale](maxscale-database-proxy.md) with at least 2 nodes to load balance requests to the [Galera](galera-cluster.md) cluster.
* Use [dedicated nodes](high-availability.md#dedicated-nodes) to avoid noisy neighbours.
* Define [pod disruption budgets](high-availability.md#pod-disruption-budgets).

Refer to the following sections for further detail.

## Kubernetes Services

In order to address nodes, MariaDB Enterprise Kubernetes Operator provides you with the following Kubernetes `Services`:

* `<mariadb-name>`: To be used for read requests. It will point to all nodes.
* `<mariadb-name>-primary`: To be used for write requests. It will point to a single node, the primary.
* `<mariadb-name>-secondary`: To be used for read requests. It will point to all nodes, except the primary.

Whenever the primary changes, either by the user or by the operator, both the `<mariadb-name>-primary` and `<mariadb-name>-secondary` `Services` will be automatically updated by the operator to address the right nodes.

The primary may be manually changed by the user at any point by updating the `spec.galera.primary.podIndex` field. Alternatively, automatic primary failover can be enabled by setting `spec.galera.primary.automaticFailover`, which will make the operator to switch primary whenever the primary `Pod` goes down.

## MaxScale

While Kubernetes `Services` can be utilized to dynamically address primary and secondary instances, the most robust high availability configuration we recommend relies on MaxScale. Please refer to [MaxScale docs](maxscale-database-proxy.md) for further detail.

## Pod Anti-Affinity

{% hint style="warning" %}
Bear in mind that, when enabling this, you need to have at least as many `Nodes` available as the replicas specified. Otherwise your `Pods` will be unscheduled and the cluster won't bootstrap.
{% endhint %}

To achieve real high availability, we need to run each `MariaDB` `Pod` in different Kubernetes `Nodes`. This practice, known as anti-affinity, helps reducing the blast radius of `Nodes` being unavailable.

By default, anti-affinity is disabled, which means that multiple `Pods` may be scheduled in the same `Node`, something not desired in HA scenarios.

You can selectively enable anti-affinity in all the different `Pods` managed by the `MariaDB` resource:

```yaml
apiVersion: enterprise.mariadb.com/v1alpha1
kind: MariaDB
metadata:
  name: mariadb-galera
spec:
  bootstrapFrom:
    restoreJob:
      affinity:
        antiAffinityEnabled: true
  ...
  metrics:
    exporter:
      affinity:
        antiAffinityEnabled: true
  ...
  affinity:
    antiAffinityEnabled: true
```

Anti-affinity may also be enabled in the resources that have a reference to `MariaDB`, resulting in their `Pods` being scheduled in `Nodes` where `MariaDB` is not running. For instance, the `Backup` and `Restore` processes can run in different `Nodes`:

```yaml
apiVersion: enterprise.mariadb.com/v1alpha1
kind: Backup
metadata:
  name: backup
spec:
  mariaDbRef:
    name: mariadb-galera
  ...
  affinity:
    antiAffinityEnabled: true
```

```yaml
apiVersion: enterprise.mariadb.com/v1alpha1
kind: Restore
metadata:
  name: restore
spec:
  mariaDbRef:
    name: mariadb-galera
  ...
  affinity:
    antiAffinityEnabled: true
```

In the case of `MaxScale`, the `Pods` will also be placed in `Nodes` isolated in terms of compute, ensuring isolation not only among themselves but also from the `MariaDB` `Pods`. For example, if you run a `MariaDB` and `MaxScale` with 3 replicas each, you will need 6 `Nodes` in total:

```yaml
apiVersion: enterprise.mariadb.com/v1alpha1
kind: MaxScale
metadata:
  name: maxscale-galera
spec:
  mariaDbRef:
    name: mariadb-galera
  ...
  metrics:
    exporter:
      affinity:
        antiAffinityEnabled: true
  ...
  affinity:
    antiAffinityEnabled: true
```

Default anti-affinity rules generated by the operator might not satisfy your needs, but you can always define your own rules. For example, if you want the `MaxScale` `Pods` to be in different `Nodes`, but you want them to share `Nodes` with `MariaDB`:

```yaml
apiVersion: enterprise.mariadb.com/v1alpha1
kind: MaxScale
metadata:
  name: maxscale-galera
spec:
  mariaDbRef:
    name: mariadb-galera
  ...
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/instance
            operator: In
            values:
            - maxscale-galera
            # 'mariadb-galera' instance omitted (default anti-affinity rule)
        topologyKey: kubernetes.io/hostname
```

## Dedicated Nodes

If you want to avoid noisy neighbours running in the same Kubernetes `Nodes` as your `MariaDB`, you may consider using dedicated `Nodes`. For achieving this, you will need:

* Taint your `Nodes` and add the counterpart toleration in your `Pods`.

{% hint style="info" %}
Tainting your `Nodes` is not covered by this operator, it is something you need to do by yourself beforehand. You may take a look at the [Kubernetes documentation](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) to understand how to achieve this.
{% endhint %}

* Select the `Nodes` where `Pods` will be scheduled in via a `nodeSelector`.

{% hint style="info" %}
Although you can use the default `Node` labels, you may consider adding more significative labels to your `Nodes`, as you will have to set to them in your `Pod` `nodeSelector`. Refer to the [Kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node).
{% endhint %}

* Add `podAntiAffinity` to your `Pods` as described in the [Pod Anti-Affinity](high-availability.md#pod-anti-affinity) section.

The previous steps can be achieved by setting these fields in the `MariaDB` resource:

```yaml
apiVersion: enterprise.mariadb.com/v1alpha1
kind: MariaDB
metadata:
  name: mariadb-galera
spec:
  ...
  tolerations:
    - key: "enterprise.mariadb.com/ha"
      operator: "Exists"
      effect: "NoSchedule"
  nodeSelector:
    "enterprise.mariadb.com/node": "ha" 
  affinity:
    antiAffinityEnabled: true
```

## Pod Disruption Budgets

{% hint style="info" %}
Take a look at the [Kubernetes documentation](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) if you are unfamiliar to `PodDisruptionBudgets`
{% endhint %}

By defining a `PodDisruptionBudget`, you are telling Kubernetes how many `Pods` your database tolerates to be down. This quite important for planned maintenance operations such as `Node` upgrades.

MariaDB Enterprise Kubernetes Operator creates a default `PodDisruptionBudget` if you are running in HA, but you are able to define your own by setting:

```yaml
apiVersion: enterprise.mariadb.com/v1alpha1
kind: MariaDB
metadata:
  name: mariadb-galera
spec:
  ...
    podDisruptionBudget:
      maxUnavailable: 33%
```

{% include "https://app.gitbook.com/s/SsmexDFPv2xG2OTyO5yV/~/reusable/pNHZQXPP5OEz2TgvhFva/" %}

{% @marketo/form formId="4316" %}
